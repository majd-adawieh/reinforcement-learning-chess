{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from env import *\n",
    "from agents.DQN import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv()\n",
    "\n",
    "model = DQN()\n",
    "model_target = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "critic_value_history = []\n",
    "action_probs_history = []\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 200\n",
    "num_actions = 4096\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "max_memory_length = 10000\n",
    "update_after_actions = 4\n",
    "update_target_network = 100\n",
    "loss_function = keras.losses.Huber()\n",
    "len_episodes = 0\n",
    "iterations = 300\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -89.50 at episode 8, frame count 100\n",
      "running reward: -87.50 at episode 14, frame count 200\n",
      "running reward: -87.64 at episode 22, frame count 300\n",
      "running reward: -86.96 at episode 28, frame count 400\n",
      "running reward: -87.11 at episode 35, frame count 500\n",
      "running reward: -87.33 at episode 43, frame count 600\n",
      "running reward: -87.20 at episode 50, frame count 700\n",
      "running reward: -86.47 at episode 55, frame count 800\n",
      "running reward: -86.22 at episode 60, frame count 900\n",
      "running reward: -86.36 at episode 67, frame count 1000\n",
      "running reward: -86.43 at episode 75, frame count 1100\n",
      "running reward: -86.38 at episode 82, frame count 1200\n",
      "running reward: -86.33 at episode 88, frame count 1300\n",
      "running reward: -86.41 at episode 95, frame count 1400\n",
      "running reward: -86.31 at episode 101, frame count 1500\n",
      "running reward: -86.04 at episode 108, frame count 1600\n",
      "running reward: -86.27 at episode 117, frame count 1700\n",
      "running reward: -86.29 at episode 123, frame count 1800\n",
      "running reward: -86.10 at episode 129, frame count 1900\n",
      "running reward: -86.07 at episode 136, frame count 2000\n",
      "running reward: -85.97 at episode 143, frame count 2100\n",
      "running reward: -86.50 at episode 151, frame count 2200\n",
      "running reward: -86.55 at episode 158, frame count 2300\n",
      "running reward: -86.42 at episode 164, frame count 2400\n",
      "running reward: -86.28 at episode 170, frame count 2500\n",
      "running reward: -86.16 at episode 176, frame count 2600\n",
      "running reward: -86.53 at episode 185, frame count 2700\n",
      "running reward: -86.61 at episode 193, frame count 2800\n",
      "running reward: -86.76 at episode 200, frame count 2900\n",
      "running reward: -86.90 at episode 206, frame count 3000\n",
      "running reward: -86.59 at episode 212, frame count 3100\n",
      "running reward: -86.41 at episode 220, frame count 3200\n",
      "running reward: -86.72 at episode 227, frame count 3300\n",
      "running reward: -86.72 at episode 234, frame count 3400\n",
      "running reward: -86.66 at episode 240, frame count 3500\n",
      "running reward: -86.46 at episode 248, frame count 3600\n",
      "running reward: -86.69 at episode 256, frame count 3700\n",
      "running reward: -86.75 at episode 262, frame count 3800\n",
      "running reward: -86.87 at episode 269, frame count 3900\n",
      "running reward: -86.85 at episode 275, frame count 4000\n",
      "running reward: -86.54 at episode 282, frame count 4100\n",
      "running reward: -86.35 at episode 288, frame count 4200\n",
      "running reward: -86.79 at episode 298, frame count 4300\n"
     ]
    }
   ],
   "source": [
    "for _ in range(iterations):\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "    len_episodes += 1\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            move, action = model.explore(env)\n",
    "        else:\n",
    "            move, action = model.predict(env)\n",
    "\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(move)\n",
    "\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        state_samples = []\n",
    "        masks = []\n",
    "        updated_q_values = []\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            indices = np.random.choice(\n",
    "                range(len(done_history)), size=batch_size)\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array(\n",
    "                [state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices])\n",
    "            future_rewards = model_target.model.predict(state_next_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            updated_q_values = updated_q_values * \\\n",
    "                (1 - done_sample) - done_sample\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model.model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.model.trainable_variables)\n",
    "            optimizer.apply_gradients(\n",
    "                zip(grads, model.model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.model.set_weights(model.model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dcb98b0cb66e7bb516e35d61dc361f03b9d6b6239965800e2e49f08121a080a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
