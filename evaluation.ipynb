{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "chess_dict = {\n",
    "    'P_l': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'P_r': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'p':   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'N_l': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'N_r': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'n':   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'B_l': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'B_r': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'b':   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'R_l': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    'R_r': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    'r':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'q':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'Q':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'k':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    'K':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    '.':   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "}\n",
    "\n",
    "num2move = {}\n",
    "move2num = {}\n",
    "\n",
    "counter = 0\n",
    "for from_sq in range(64):\n",
    "    for to_sq in range(64):\n",
    "        num2move[counter] = chess.Move(from_sq, to_sq)\n",
    "        move2num[chess.Move(from_sq, to_sq)] = counter\n",
    "        counter += 1\n",
    "        \n",
    "        \n",
    "def translate_board(board):\n",
    "    pgn = board.epd()\n",
    "    foo = []\n",
    "    pieces = pgn.split(\" \", 1)[0]\n",
    "    rows = pieces.split(\"/\")\n",
    "    for row in rows:\n",
    "        foo2 = []\n",
    "        for index, thing in enumerate(row):\n",
    "            if thing.isdigit():\n",
    "                for i in range(0, int(thing)):\n",
    "                    foo2.append(chess_dict['.'])\n",
    "            else:\n",
    "                if thing not in [\"P\", \"N\", \"R\", \"B\"]:\n",
    "                    foo2.append(chess_dict[thing])\n",
    "                else:\n",
    "                    if(index < 4):\n",
    "                        foo2.append(chess_dict[thing+\"_l\"])\n",
    "                    else:\n",
    "                        foo2.append(chess_dict[thing+\"_r\"])\n",
    "        foo.append(foo2)\n",
    "    return np.array(foo)\n",
    "        \n",
    "def find_piece_key(piece_representation):\n",
    "    index = np.argmax(piece_representation)\n",
    "    piece_key = \"\"\n",
    "    for key in chess_dict:\n",
    "        if index == np.argmax(chess_dict[key]):\n",
    "            piece_key = key\n",
    "            break\n",
    "    return piece_key\n",
    "\n",
    "\n",
    "def can_move(move, agent_num, translated_board):\n",
    "    from_square = move.from_square\n",
    "    piece = translated_board[7-from_square//8][from_square % 8]\n",
    "    piece_key = find_piece_key(piece)\n",
    "    if agent_num == 0 and \"_l\" in piece_key:\n",
    "        return True\n",
    "    if agent_num == -1 and \"_r\" in piece_key:\n",
    "        return True\n",
    "    if \"Q\" in piece_key or \"K\" in piece_key:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_legal_moves(board, logits, agent_num, translated_board):\n",
    "    filter_mask = np.zeros(logits.shape)\n",
    "    legal_moves = board.legal_moves\n",
    "    num_legal_mobes = list(board.legal_moves)\n",
    "    for legal_move in legal_moves:\n",
    "        if agent_num is not None:\n",
    "            if(can_move(legal_move, agent_num, translated_board)):\n",
    "                from_square = legal_move.from_square\n",
    "                to_square = legal_move.to_square\n",
    "                idx = move2num[chess.Move(from_square, to_square)]\n",
    "                filter_mask[idx] = 1\n",
    "        else:\n",
    "            from_square = legal_move.from_square\n",
    "            to_square = legal_move.to_square\n",
    "            idx = move2num[chess.Move(from_square, to_square)]\n",
    "            filter_mask[idx] = 1\n",
    "    new_logits = logits*filter_mask\n",
    "    return new_logits\n",
    "\n",
    "\n",
    "def check_legal_move(board, move):\n",
    "    legal_moves = board.legal_moves\n",
    "    legal = False\n",
    "    for legal_move in legal_moves:\n",
    "        from_square = legal_move.from_square\n",
    "        to_square = legal_move.to_square\n",
    "        if from_square == move.from_square and to_square == move.to_square:\n",
    "            legal = True\n",
    "            break\n",
    "\n",
    "    return legal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent_dqn(nn.Module):\n",
    "  def __init__(self, obs_shape, n_actions):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(obs_shape[0], 64, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(64, 128, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(128, 256, kernel_size=2,stride=2),\n",
    "    )\n",
    "    conv_out_size = self._get_conv_out(obs_shape)\n",
    " \n",
    "    self.fc = nn.Linear(conv_out_size, n_actions)\n",
    "\n",
    "  def _get_conv_out(self, shape):\n",
    "    conv_out = self.conv(torch.zeros(1, *shape))\n",
    "    return int(np.prod(conv_out.size()))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
    "    x = self.fc(x)\n",
    "    return torch.softmax(x, dim=1)\n",
    "\n",
    "class agent_double_dqn(nn.Module):\n",
    "  def __init__(self, obs_shape, n_actions):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(obs_shape[0], 64, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(64, 128, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(128, 256, kernel_size=2,stride=2),\n",
    "    )\n",
    "    conv_out_size = self._get_conv_out(obs_shape)\n",
    " \n",
    "    self.fc = nn.Linear(conv_out_size, n_actions)\n",
    "\n",
    "  def _get_conv_out(self, shape):\n",
    "    conv_out = self.conv(torch.zeros(1, *shape))\n",
    "    return int(np.prod(conv_out.size()))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
    "    x = self.fc(x)\n",
    "    return torch.softmax(x, dim=1)\n",
    "\n",
    "class agent_duelling_dqn(nn.Module):\n",
    "  def __init__(self, obs_shape, n_actions):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(obs_shape[0], 64, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(64, 128, kernel_size=2,stride=2),\n",
    "        nn.Conv2d(128, 256, kernel_size=2,stride=2),\n",
    "    )\n",
    "    conv_out_size = self._get_conv_out(obs_shape)\n",
    "    \n",
    "    self.fc = nn.Linear(conv_out_size, 512)\n",
    "    self.fc_adv = nn.Linear(512, n_actions) \n",
    "    self.fc_value = nn.Linear(512, 1)\n",
    "\n",
    "  def _get_conv_out(self, shape):\n",
    "    conv_out = self.conv(torch.zeros(1, *shape))\n",
    "    return int(np.prod(conv_out.size()))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
    "    x = self.fc(x)\n",
    "    adv = self.fc_adv(x)\n",
    "    value = self.fc_value(x)\n",
    "    x = value + adv - torch.mean(adv, dim=1, keepdim=True)\n",
    "    return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, env, q_network, agent_num = None, epsilon=0.):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        action_probs = 1 * 2 + np.random.uniform(0, 1, 4096)\n",
    "        action_space = filter_legal_moves(env.board, action_probs, agent_num, env.translated_board)\n",
    "        action_space = torch.from_numpy(action_space)\n",
    "        action = torch.argmax(action_space, dim=-1, keepdim=True)\n",
    "        move = num2move[action.item()]\n",
    "        return action, move\n",
    "    else:\n",
    "        action_probs = q_network(state).detach()\n",
    "        action_space = filter_legal_moves(env.board, action_probs[0], agent_num, env.translated_board)\n",
    "        action =  torch.argmax(action_space, dim=-1, keepdim=True)\n",
    "\n",
    "        move = num2move[action.item()]\n",
    "        return action, move\n",
    "\n",
    "class ChessEnv():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = chess.Board()\n",
    "        self.translated_board = translate_board(self.board)\n",
    "        self.next_agent = 0\n",
    "        return self.translated_board\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.board\n",
    "\n",
    "    def get_next_agent(self):\n",
    "        return self.next_agent\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.board.push(action)\n",
    "        self.update_translated_borad(action)\n",
    "        self.next_agent = ~self.next_agent\n",
    "        state_next = self.translated_board\n",
    "        self.done = self.board.is_checkmate() \n",
    "        if chess.Status.PAWNS_ON_BACKRANK == self.board.status():\n",
    "            self.done = True \n",
    "        is_game_over = self.board.is_insufficient_material()\n",
    "        return state_next, None, self.done, None, is_game_over\n",
    "    \n",
    "    \n",
    "    def preprocess(self, board):\n",
    "        pgn = board.epd()\n",
    "        processed_board = []\n",
    "        pieces = pgn.split(\" \", 1)[0]\n",
    "        rows = pieces.split(\"/\")\n",
    "        \n",
    "        for row in rows:\n",
    "            processed_row = []\n",
    "            for index, element in enumerate(row):\n",
    "                if element.isdigit():\n",
    "                    for i in range(0, int(element)):\n",
    "                        processed_row.append(chess_dict['.'])\n",
    "                else:\n",
    "                    if element not in [\"P\", \"N\", \"R\", \"B\"]:\n",
    "                        processed_row.append(chess_dict[element])\n",
    "                    else:\n",
    "                        if(index < 4):\n",
    "                            processed_row.append(chess_dict[element+\"_l\"])\n",
    "                        else:\n",
    "                            processed_row.append(chess_dict[element+\"_r\"])\n",
    "            processed_board.append(processed_row)\n",
    "        return np.array(processed_board)\n",
    "    \n",
    "    def update_translated_borad(self, action):\n",
    "        from_square = action.from_square\n",
    "        to_square = action.to_square\n",
    "        tmp = self.translated_board[7-from_square//8][from_square % 8].copy()\n",
    "        self.translated_board[7-from_square//8][from_square %\n",
    "                                                8] = self.translated_board[7-to_square//8][to_square % 8]\n",
    "        self.translated_board[7-to_square//8][to_square % 8] = tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv()\n",
    "state = env.reset()\n",
    "obs_size = state.shape\n",
    "num_actions = 4096\n",
    "\n",
    "def load(path, n_actions, state_size, model_name):\n",
    "    if model_name == \"DQN\":\n",
    "            model = agent_dqn(n_actions,state_size).to(device)\n",
    "    if model_name == \"DOUBLE_DQN\":\n",
    "            model = agent_double_dqn(n_actions,state_size).to(device)\n",
    "    if model_name == \"DUELLING_DQN\":\n",
    "            model = agent_duelling_dqn(n_actions,state_size).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(episodes, team1_path, team1_name,  team2_path, team2_name):\n",
    "    agent1 =  load(team1_path, obs_size,num_actions, team1_name)\n",
    "    agent2 =  load(team1_path, obs_size,num_actions, team1_name)\n",
    "    agent3 =  load(team2_path,  obs_size,num_actions, team2_name)\n",
    "    results = {\"team_1_wins\":0, \"team_2_wins\":0, \"draw\":0}\n",
    "    for i in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(state).unsqueeze(dim=0)\n",
    "        next_agent = env.get_next_agent()\n",
    "        done = False\n",
    "        game_over = False\n",
    "        agent_num = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not done and not game_over:\n",
    "            step += 1\n",
    "            state = state.float().to(device)\n",
    "            if next_agent == 0:\n",
    "                action, move = policy(state,env, agent1, None, 0.05)\n",
    "                agent_num = ~agent_num\n",
    "                if action == 0:\n",
    "                        action, move = policy(state,env, agent2, ~agent_num, 0.05)\n",
    "            else:\n",
    "                action, move = policy(state,env, agent3, None, 0.05)\n",
    "        \n",
    "            next_state, _, done, _, game_over = env.step(move)\n",
    "            if game_over:\n",
    "                results[\"draw\"] += 1\n",
    "            if  done:\n",
    "                if agent_num == 0:\n",
    "                    results[\"team_1_wins\"] += 1\n",
    "                else:\n",
    "                    results[\"team_2_wins\"] += 1\n",
    "            next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
    "            state = next_state\n",
    "            next_agent = env.get_next_agent()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:30<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-deep-q-learning one-agent-deep-q-learning {'team_1_wins': 95, 'team_2_wins': 94, 'draw': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-deep-q-learning one-agent-double-q-learning {'team_1_wins': 96, 'team_2_wins': 97, 'draw': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [17:08<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-deep-q-learning one-agent-duelling-q-learning {'team_1_wins': 88, 'team_2_wins': 101, 'draw': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [19:08<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-double-q-learning one-agent-deep-q-learning {'team_1_wins': 90, 'team_2_wins': 95, 'draw': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [15:18<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-double-q-learning one-agent-double-q-learning {'team_1_wins': 92, 'team_2_wins': 93, 'draw': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [17:04<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-double-q-learning one-agent-duelling-q-learning {'team_1_wins': 88, 'team_2_wins': 89, 'draw': 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:25<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-duelling-q-learning one-agent-deep-q-learning {'team_1_wins': 91, 'team_2_wins': 105, 'draw': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:32<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-duelling-q-learning one-agent-double-q-learning {'team_1_wins': 92, 'team_2_wins': 94, 'draw': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:32<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-agent-duelling-q-learning one-agent-duelling-q-learning {'team_1_wins': 96, 'team_2_wins': 91, 'draw': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run(200, \"./models/multi-agent-deep-q-learning\",\"DQN\", \"./models/one-agent-deep-q-learning\", \"DQN\")\n",
    "print(\"multi-agent-deep-q-learning\",\"one-agent-deep-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-deep-q-learning\",\"DQN\", \"./models/one-agent-double-q-learning\", \"DOUBLE_DQN\")\n",
    "print(\"multi-agent-deep-q-learning\",\"one-agent-double-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-deep-q-learning\",\"DQN\", \"./models/one-agent-duelling-q-learning\", \"DUELLING_DQN\")\n",
    "print(\"multi-agent-deep-q-learning\",\"one-agent-duelling-q-learning\",results)\n",
    "\n",
    "results = run(200, \"./models/multi-agent-double-q-learning\",\"DOUBLE_DQN\", \"./models/one-agent-deep-q-learning\", \"DQN\")\n",
    "print(\"multi-agent-double-q-learning\",\"one-agent-deep-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-double-q-learning\",\"DOUBLE_DQN\", \"./models/one-agent-double-q-learning\", \"DOUBLE_DQN\")\n",
    "print(\"multi-agent-double-q-learning\",\"one-agent-double-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-double-q-learning\",\"DOUBLE_DQN\", \"./models/one-agent-duelling-q-learning\", \"DUELLING_DQN\")\n",
    "print(\"multi-agent-double-q-learning\",\"one-agent-duelling-q-learning\",results)\n",
    "\n",
    "results = run(200, \"./models/multi-agent-duelling-q-learning\",\"DUELLING_DQN\", \"./models/one-agent-deep-q-learning\", \"DQN\")\n",
    "print(\"multi-agent-duelling-q-learning\",\"one-agent-deep-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-duelling-q-learning\",\"DUELLING_DQN\", \"./models/one-agent-double-q-learning\", \"DOUBLE_DQN\")\n",
    "print(\"multi-agent-duelling-q-learning\",\"one-agent-double-q-learning\",results)\n",
    "results = run(200, \"./models/multi-agent-duelling-q-learning\",\"DUELLING_DQN\", \"./models/one-agent-duelling-q-learning\", \"DUELLING_DQN\")\n",
    "print(\"multi-agent-duelling-q-learning\",\"one-agent-duelling-q-learning\",results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dcb98b0cb66e7bb516e35d61dc361f03b9d6b6239965800e2e49f08121a080a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
